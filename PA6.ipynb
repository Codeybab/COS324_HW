{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Codeybab/COS324_HW/blob/main/PA6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F62B_4pcRI-"
      },
      "source": [
        "# Programming Assignment 6: Value Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "052JVvHvcbQ9"
      },
      "source": [
        "## Problem Setting\n",
        "\n",
        "**Gridworld**: In this assignment, we use the popular RL toolkit  [OpenAI Gym](https://gym.openai.com/)  to implement _value iteration_ on a gridworld environment (Lecture 21). In the gridworld environment we consider, an agent needs to navigate on a 2-D plane by using 4 actions (left, right, up, and down). It starts at a designated state (starting point \"S\"), and needs to try and reach a goal state (\"G\"). The agent controls the movement of a character in a grid world. Some tiles on the grid are walkable (\"S\", \"G\", and \"F\"), and others (\"H\") lead to the agent falling into the water, which ends the episode. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
        "\n",
        "**Rewards**: The environment is such that reaching the goal state gives the agent a `+1` reward, and it gets a `0` reward in other states (detailed description below). Hence, reaching the goal state is equivalent to maximizing the rewards it can accumulate.\n",
        "\n",
        "**Uncertainty**: The environment also has some uncertainty. That is, the movement direction of the agent is uncertain and only partially depends on the chosen direction. In our setting (detailed below), each tile in the grid is \"slippery\", which corresponds to the following uncertainty; if the agent chose to move up/down, it will be able to do so with probability `1/3`, but with probability `2/3` it will move right/left (`1/3` right, `1/3` left). Similarly for the case that the agent chose to move right/left, it might move up/down instead. However, you will always move by 1 tile only (i.e., no \"jumps\"). \n",
        "\n",
        "This is visualized below:\n",
        "\n",
        "<img src=\"fig/penguin_walk.png\">\n",
        "\n",
        "In a) the agent (penguin) tries to walk down, in b) it tries to walk right, and c) shows a potential pitfall where just by walking by a hole \"H\" (and not directly into it), the agent still has a `1/3` chance to fall into the hole, which ends the episode with no reward.\n",
        "\n",
        "\n",
        "Note that: \n",
        "* The specific transition probabilities don't matter much here, but it is important to keep in mind is that the action you choose to move towards only partially determined where you actually move, due to the uncertianty (\"slipperiness\") of the environment. \n",
        "\n",
        "* Recall that while you can see the whole gridworld (all the states), the agent only has information about its current state!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc061mOonRbf"
      },
      "source": [
        "## Environment: Frozen Lake \n",
        "Winter is here. You and your friends are tossing around a frisbee at the park when you make a wild throw that leaves the frisbee stranded out in the middle of\n",
        "a lake. The water is mostly frozen (\"F\"), but there are a few holes (\"H\") where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend, and sometimes slip into another, adjacent tile! The surface is described using a grid like the following $8 \\times 8$ example:\n",
        "\n",
        "    SFFFFFFF\n",
        "    FFFFFFFF\n",
        "    FFFHFFFF\n",
        "    FFFFFHFF\n",
        "    FFFHFFFF\n",
        "    FHHFFFHF\n",
        "    FHFFHFHF\n",
        "    FFFHFFFG\n",
        "\n",
        "\n",
        "    S : starting point, safe\n",
        "    F : frozen surface, safe\n",
        "    H : hole, fall to your doom\n",
        "    G : goal, where the frisbee is located\n",
        "\n",
        "\n",
        "The episode ends when you reach the goal or fall in a hole \"H\". You receive a reward of 1 if you reach the goal, and 0 otherwise. The steps that you can make are one of the following: LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3.\n",
        "\n",
        "## Visualization:\n",
        "It is easier to understand how this environment behaves if you see it first.  \n",
        "\n",
        "#### $\\star$ We recommend that before implementing anything, you start by first running all cells of this notebook, and scroll down to see how a **totally-random** agent moves around the frozen lake. $\\star$ \n",
        " \n",
        "You will see it fail many times. In what follows, we will implement a much better agent that can navigate to the goal.\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR-2uWK2nRbg"
      },
      "source": [
        "### Notation\n",
        "\n",
        "Mapping of the OpenAI gym constructs to the notation used in lectures:\n",
        "* The value function $V(s)$, is implemented here as a small array, of size corrsponsing to number of states, such that `V[s]` contains the value $V(s)$. \n",
        "* The transition probabilities of the environment $P(s'|s,a)$ correspond to the gym object `env.P` described below. \n",
        "* In lectures, the reward function $r(a|s, s')$ signified  taking action $a$ while in state $s$ moving to state $s'$. However, in our setting here the reward fixed into the environment is such that $r(a|s=\\text{'G'}, s')=1$ and $r(a|s \\neq\\text{'G'}, s')=0$ for any $a, s'$. In other words, in this setting we only get the reward if we *actually reach* the goal state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNaDE1zyrL2"
      },
      "source": [
        "### First, install dependencies (takes ~45 seconds)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-AxnvAVyzQQ"
      },
      "outputs": [],
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb libav-tools python-opengl ffmpeg > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdb2JwZy4jGj"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGEFMfDOzLen"
      },
      "outputs": [],
      "source": [
        "# first, we set some useful parameters: \n",
        "gamma = 0.99        # discount factor\n",
        "theta = 0.000001    # precision of value_iteration\n",
        "        \n",
        "# let's set up the frozen lake environment. \n",
        "try:\n",
        "    env = gym.make(\"FrozenLake8x8-v0\")\n",
        "except:\n",
        "    env = gym.make(\"FrozenLake8x8-v1\") # v0 might not exist\n",
        "    \n",
        "num_states = env.nS   # we have one state per tile in the grid (16 states)\n",
        "num_actions = env.nA  # we can move up/down/left/right, i.e., 4 actions. \n",
        "\n",
        "# NOTE, the transition probabilities we mentioned above (i.e., the \"slipperiness\"), are already given \n",
        "# in the env.P object. Specifically, env.P are the transition probabilities (dictionary dict of dicts of lists)\n",
        "#           P[s][a] == [(probability, s', reward, done), \n",
        "#                       (probability, s', reward, done), \n",
        "#                       (probability, s', reward, done), ]\n",
        "\n",
        "# As an example, for state s=0 (the starting tile \"S\"),and action a=0 (LEFT), you will have:\n",
        "#           P[0][0] == [(1/3, 0, 0, False), (1/3, 0, 0, False), (1/3, 4, 0, False)] \n",
        "# since:\n",
        "# with probability 1/3 you'll move UP (but there's no up, so you'll stay put at s=0),\n",
        "# with probability 1/3 you'll move LEFT (but there's no left, so you'll stay put at s=0),\n",
        "# with probability 1/3 you'll move DOWN (so you'll get to the tile below, which is s'=4).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDP6Ff27k_uk"
      },
      "source": [
        "We will now implement the Value Iteration algorithm you have seen in class. The pseudo-code is given below: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmvxaVfonRbj"
      },
      "source": [
        "1. Parameter: a small threshold $\\theta>0$ determining accuracy of estimation.\n",
        "1. Initialize $V(s)=-\\frac{M}{1-\\gamma}$, for all $s\\in\\mathcal{S}$, where $M$ is the upper bound on the absolute value of immediate rewards.\n",
        "1. **Loop**:\n",
        "    1. $\\Delta \\leftarrow 0$.\n",
        "    1. **Loop for each** $s\\in\\mathcal{S}$:\n",
        "        1. $v\\leftarrow V(s)$.\n",
        "        1. $V(s) \\leftarrow \\max_a\\sum_{s'} p(s'\\mid s, a)\\bigl[r(a\\mid s, s') + \\gamma V(s')\\bigr]$.\n",
        "        1. $\\Delta\\leftarrow \\max(\\Delta, \\lvert v - V(s) \\rvert)$.\n",
        "1. **until** $\\Delta < \\theta$.\n",
        "1. Output a deterministic policy, $\\pi\\approx\\pi_*$, such that\n",
        "$$ \\pi(s) = \\text{argmax}_a \\sum_{s'} p(s'\\mid s, a)\\bigl[r(a\\mid s, s') + \\gamma V(s')\\bigr].$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA5zx3F1nRbj"
      },
      "source": [
        "#### ACTS 1-2: Computing the sum & argmax of Value-Iteration. \n",
        "\n",
        "Compute the inner sum $$\\sum_{s'} p(s'\\mid s, a)\\bigl[r(a\\mid s, s') + \\gamma V(s')\\bigr],$$ given $V, s, a, \\gamma$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5topu9WrLNo_"
      },
      "outputs": [],
      "source": [
        "# ACT 1: compute the inner sum of the equations above, given:\n",
        "        # Value function V (an array of size num_states),\n",
        "        # State s, Action a, and discount factor gamma.\n",
        "        \n",
        "def sum_over_states(V, s, a, gamma):\n",
        "    sum_val = 0\n",
        "    # recall that env.P is a list of tuples for (s,a): [(prob, s', r, done),...] (See code-block above for more details!)\n",
        "    # where 'prob' is the transition probability P(s'|s,a). \n",
        "    # also, since r is associated with the states, you can think of the sum as only traversing states s'. \n",
        "    # therefore, all is needed is to iterate over each possible transition to state s'\n",
        "    # from the pair (s,a), and compute the sum value (as in the pseudo-code).\n",
        "    temp_prob = env.P[s][a]\n",
        "    for i in range(len(temp_prob)): \n",
        "      sum_val += temp_prob[i][0] * (temp_prob[i][2] + gamma * V(temp_prob[i][1]))\n",
        "    return sum_val\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caUxLJOAnRbk"
      },
      "source": [
        "We now compute the argmax operation, in which we will update $\\pi(a|s)$ where $s$ is the given state, and $a$ is the maximizing action! To obtain it, you should use the function above `sum_over_states`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_n38Ros7nRbk"
      },
      "outputs": [],
      "source": [
        "# ACT 2: update the action which can take us to a higher valued state, given:\n",
        "        # Value function V (an array of size num_states),\n",
        "        # Policy array pi (a matrix of shape [num_states, num_actions]),\n",
        "        # State s, and discount factor gamma.\n",
        "        \n",
        "def argmax_over_actions(V, pi, s, gamma):\n",
        "    max_val = 0\n",
        "    temp_arr = []\n",
        "    min_index = 0\n",
        "    for a in range(num_actions): \n",
        "      temp_arr.append(sum_over_states(V,s,a,gamma))\n",
        "    max_val = max(temp_arr)\n",
        "    return pi, max_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YI14dtPmnRbk"
      },
      "source": [
        "#### ACTS 3: Computing the Bellman update\n",
        "\n",
        "We now perform a single iteration of update to the value function $V$, given state $s$, and discount factor $\\gamma$. \n",
        "You may call a function you have implemented above, and output the difference between the new, updated value for this state, and the previous value (denoted as `delta` in the pseudo-code above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TADCWSLfXB4"
      },
      "outputs": [],
      "source": [
        "# ACT 3: update the Value function for state s, that is: V[s], by taking action which maximizes current value. Given:\n",
        "        # Value function V (an array of size num_states),\n",
        "        # State s, and discount factor gamma.\n",
        "\n",
        "def bellman_optimality_update(V, s, gamma):  \n",
        "    pi = np.zeros((num_states, num_actions))     \n",
        "    v = V[s]\n",
        "    pi, max_val = argmax_over_actions(V, pi, s, gamma)\n",
        "    action_max = np.argmax(pi[s])\n",
        "    V[s] = max_val\n",
        "    delta = np.abs(v - V[s])\n",
        "        # ACT3a: call a function implemented above, to get the action which maximizes current value.\n",
        "        # ACT3b: update value V[s]\n",
        "        # ACT 3c: compute and output delta.\n",
        "    return delta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LK7qV3XnRbl"
      },
      "source": [
        "#### ACT 4: Initialize V\n",
        "Initialize $V = -\\frac{M}{1-\\gamma}$, where $M$ is the upper bound on the absolute value of immediate rewards to be filled in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVeNB8GanRbl"
      },
      "outputs": [],
      "source": [
        "# ACT 4: Initialize V\n",
        "# length_V gives the size of the vector V\n",
        "def init_V(gamma, M, length_V):\n",
        "    ### YOUR CODE GOES HERE\n",
        "    V = np.ones(length_V) * (-1 * M/(1 - gamma))\n",
        "    return V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juXeTQCFnRbl"
      },
      "source": [
        "#### ACT 5-6: Value-iteration (putting it all together)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsGJ0nSieiix"
      },
      "outputs": [],
      "source": [
        "# Now, let's put it all together. Recall that we are given:\n",
        "        # discount factor gamma, and wanted precision theta.\n",
        "def value_iteration(gamma, theta):\n",
        "    # Initialize V with init_V function\n",
        "    ### YOUR CODE GOES HERE\n",
        "    V = init_V(gamma, 1, num_states)\n",
        "    # ACT 5: constrct the main loop, \n",
        "    #        iteratively calling the bellman update,\n",
        "    #        and break when sufficient accuracy was reached. \n",
        "    while True: \n",
        "        delta = 0\n",
        "        for s in range(num_states): \n",
        "          prev_delta = delta\n",
        "          delta = max(prev_delta, bellman_optimality_update(V,s,gamma))\n",
        "        if(delta < theta): \n",
        "          break\n",
        "    pi = np.zeros((num_states, num_actions))\n",
        "\n",
        "    pi = np.zeros((num_states, num_actions)) \n",
        "    # ACT 6: Extract optimal policy\n",
        "    for s in range(num_states): \n",
        "      pi, _ = argmax_over_actions(V, pi, s, gamma)\n",
        "        \n",
        "    # output optimal value funtion, optimal policy\n",
        "    return V, pi                                  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugmVlSvSnRbm"
      },
      "source": [
        "This next function `pretty_print` is just used to get some info printed out during the run.\n",
        "\n",
        "Nothing for you to do here... "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2GuhYhn4Gtb"
      },
      "outputs": [],
      "source": [
        "def pretty_print(i_episode, t, done, reward, total_rewards):\n",
        "    print(\"Trial=\" + str(i_episode)), print(\"Step=\" + str(t)), print('')\n",
        "    env.render()\n",
        "    if done:\n",
        "        print('')\n",
        "        if reward == 1:\n",
        "            total_rewards += 1\n",
        "            print(\">> Success! got the goal\")\n",
        "        else:\n",
        "            print(\">> Failed! fell into a hole\")\n",
        "        time.sleep(3), clear_output(wait=True)\n",
        "    else:\n",
        "      time.sleep(.5), clear_output(wait=True)\n",
        "    it = max(i_episode, 1)\n",
        "    print(\"Avg reward so far = %.2f\" % (total_rewards / it))\n",
        "    print('')\n",
        "    return total_rewards\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UenSgosHnRbm"
      },
      "source": [
        "#### ACT 7: Call agent, and evaluate results\n",
        "\n",
        "Below is the main loop of our setting: first, we call the `value_iteration` function, and obtain the optimal policy. \n",
        "Then, we run in many episodes (or trials) of the algorithm. In each trial, we start at the starting state \"S\". We then ask the agent what action to go towards. We make a step in the frozen lake with the chosen action (but may not move there because the lake is slippery!), our next state is given after we call `env.step(action)`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nj5sjsk15IT"
      },
      "outputs": [],
      "source": [
        "# note that you can first try it out with a random agent, by setting this variable to True. \n",
        "random_agent = True\n",
        "episodes = 100\n",
        "\n",
        "if not random_agent:\n",
        "    # We have called the value_iteration function:\n",
        "    V_, pi = value_iteration(gamma, theta)\n",
        "    # note that we don't need the value function V anymore, as we already extracted the optimal policy!\n",
        "\n",
        "total_rewards = 0\n",
        "for i_episode in range(episodes):\n",
        "      state = env.reset()\n",
        "      t = 0\n",
        "      while True:\n",
        "          t+=1\n",
        "          # your agent picks an action: \n",
        "          if random_agent:\n",
        "            action = env.action_space.sample() # totally random action !\n",
        "          else:\n",
        "            # ACT 7: get the best action according to optimal policy `pi`, and current state `state`.\n",
        "            action = 0\n",
        "            action = np.argmax(pi[state])\n",
        "              \n",
        "          # make a step according to policy\n",
        "          state, reward, done, info = env.step(action) \n",
        "          # the above variables are:\n",
        "                # state (object): agent's observation, current state (new state we've reached after the step we took)\n",
        "                # reward (float) : amount of reward returned after previous action\n",
        "                # done (bool): whether the episode has ended (only True if the state is the Goal or a Hole!)\n",
        "                # info (dict): contains auxiliary diagnostic information (might be helpful for debugging, though not used in this assignment)\n",
        "\n",
        "          # visualization\n",
        "          total_rewards = pretty_print(i_episode, t, done, reward, total_rewards)\n",
        "          if done: break\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cohoX1l1nRbn"
      },
      "source": [
        "### Conceptual Questions (ACTs 8-10)\n",
        "\n",
        "Notice that in your implementation above, an agent that follows the value-iteration policy performs much better than an agent that follows the random policy (you can run both variations by setting the `random_agent` variable to `True/False`. \n",
        "\n",
        "Please **briefly** answer the following questions (1-2 sentences per question suffices):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ErUhkC4nRbn"
      },
      "source": [
        "#### Q1: Does the value-iteration policy *always* obtains reward=1? Explain why/why not.\n",
        "#### <span style='color:cyan'>  Your answer here:It does not always obtain that award. Since it still has a 1/3 probability of going the wrong way. like if the right direction is is up, still 1/3 they go down or right!  </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVEEvAa4nRbn"
      },
      "source": [
        "#### Q2: Will the random policy *always* obtain reward=0? Explain why/why not.\n",
        "#### <span style='color:cyan'>  Your answer here: again not \"always\" as there is always a possiblity that the penguin reaches the right state meaning thet obtained a reward of 1, however, random it is does not matter so long as at least once they reach the desired state. </span> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1jZVpdEnRbn"
      },
      "source": [
        "#### Q3: When averaged over many episodes, will the value-iteration policy always obtain a larger reward than the random policy? Explain why/why not (no need to prove it, just state the appropriate claim shown in lecture and breifly explain how it implies this statement). \n",
        "#### <span style='color:cyan'>  Your answer here:It will always have a bigger value since the iterations are giving the max value of discount/reward. While the accuracy may increase over the trails since the value iteration policy starts bigger and grows with iterations and version it will always stay bigger!  </span>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "PA6.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}